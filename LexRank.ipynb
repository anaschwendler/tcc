{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    numpy = None\n",
    "\n",
    "from sumy.summarizers._summarizer import AbstractSummarizer\n",
    "from sumy._compat import Counter\n",
    "\n",
    "\n",
    "class ConceptSummarizer(AbstractSummarizer):\n",
    "\n",
    "    threshold = 0.1\n",
    "    epsilon = 0.1\n",
    "    _stop_words = frozenset()\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = frozenset(map(self.normalize_word, words))\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        self._ensure_dependencies_installed()\n",
    "\n",
    "        sentences_words = [self._to_words_set(s) for s in document.sentences]\n",
    "        if not sentences_words:\n",
    "            return tuple()\n",
    "\n",
    "        tf_metrics = self._compute_tf(sentences_words)\n",
    "        idf_metrics = self._compute_idf(sentences_words)\n",
    "\n",
    "        matrix = self._create_matrix(sentences_words, self.threshold, tf_metrics, idf_metrics)\n",
    "        scores = self.power_method(matrix, self.epsilon)\n",
    "        ratings = dict(zip(document.sentences, scores))\n",
    "\n",
    "        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dependencies_installed():\n",
    "        if numpy is None:\n",
    "            raise ValueError(\"Concept summarizer requires NumPy. Please, install it by command 'pip install numpy'.\")\n",
    "\n",
    "    def _to_words_set(self, sentence):\n",
    "        words = map(self.normalize_word, sentence.words)\n",
    "        return [self.stem_word(w) for w in words if w not in self._stop_words]\n",
    "\n",
    "    def _compute_tf(self, sentences):\n",
    "        tf_values = map(Counter, sentences)\n",
    "\n",
    "        tf_metrics = []\n",
    "        for sentence in tf_values:\n",
    "            metrics = {}\n",
    "            max_tf = self._find_tf_max(sentence)\n",
    "\n",
    "            for term, tf in sentence.items():\n",
    "                metrics[term] = tf / max_tf\n",
    "\n",
    "            tf_metrics.append(metrics)\n",
    "\n",
    "        return tf_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_tf_max(terms):\n",
    "        return max(terms.values()) if terms else 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_idf(sentences):\n",
    "        idf_metrics = {}\n",
    "        sentences_count = len(sentences)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for term in sentence:\n",
    "                if term not in idf_metrics:\n",
    "                    n_j = sum(1 for s in sentences if term in s)\n",
    "                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n",
    "\n",
    "        return idf_metrics\n",
    "\n",
    "    def _create_matrix(self, sentences, threshold, tf_metrics, idf_metrics):\n",
    "        \"\"\"\n",
    "        Creates matrix of shape |sentences|×|sentences|.\n",
    "        \"\"\"\n",
    "        # create matrix |sentences|×|sentences| filled with zeroes\n",
    "        sentences_count = len(sentences)\n",
    "        matrix = numpy.zeros((sentences_count, sentences_count))\n",
    "        degrees = numpy.zeros((sentences_count, ))\n",
    "\n",
    "        for row, (sentence1, tf1) in enumerate(zip(sentences, tf_metrics)):\n",
    "            for col, (sentence2, tf2) in enumerate(zip(sentences, tf_metrics)):\n",
    "                matrix[row, col] = self.compute_distance(sentence1, sentence2, tf1, tf2, idf_metrics)\n",
    "\n",
    "                if matrix[row, col] > threshold:\n",
    "                    matrix[row, col] = 1.0\n",
    "                    degrees[row] += 1\n",
    "                else:\n",
    "                    matrix[row, col] = 0\n",
    "\n",
    "        for row in range(sentences_count):\n",
    "            for col in range(sentences_count):\n",
    "                if degrees[row] == 0:\n",
    "                    degrees[row] = 1\n",
    "\n",
    "                matrix[row][col] = matrix[row][col] / degrees[row]\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    #@staticmethod\n",
    "\n",
    "    @staticmethod\n",
    "    def power_method(matrix, epsilon):\n",
    "        transposed_matrix = matrix.T\n",
    "        sentences_count = len(matrix)\n",
    "        p_vector = numpy.array([1.0 / sentences_count] * sentences_count)\n",
    "        lambda_val = 1.0\n",
    "\n",
    "        while lambda_val > epsilon:\n",
    "            next_p = numpy.dot(transposed_matrix, p_vector)\n",
    "            lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))\n",
    "            p_vector = next_p\n",
    "\n",
    "        return p_vector\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_distance(sentence1, sentence2, tf1, tf2, idf_metrics):   \n",
    "        EPSILON = 0.0000000000000001\n",
    "        result = 0\n",
    "\n",
    "        # identify common words\n",
    "        common_words = frozenset(sentence1) & frozenset(sentence2)\n",
    "\n",
    "        if len(sentence1) > len(sentence2): \n",
    "            maxLen = len(sentence1); \n",
    "            minLen = len(sentence2) \n",
    "        else: \n",
    "            maxLen = len(sentence2); \n",
    "            minLen = len(sentence1) \n",
    "\n",
    "        # calculates similarity\n",
    "        wordWeightMax = 0; wordWeightMin = 0;\n",
    "        for term in common_words:\n",
    "            if wordWeightMax < len(term): wordWeightMax = len(term)\n",
    "            if wordWeightMin > len(term): wordWeightMin = len(term)\n",
    "            negationWordWeightMax = 1 - wordWeightMax;\n",
    "            negationWordWeightMin = 1 - wordWeightMin;\n",
    "\n",
    "            c1 = 1 if wordWeightMin == 0 else wordWeightMax / wordWeightMin;\n",
    "            c2 = 1 if wordWeightMax == 0 else wordWeightMin / wordWeightMax;\n",
    "            c3 = 1 if negationWordWeightMin == 0 else negationWordWeightMax / negationWordWeightMin;\n",
    "            c4 = 1 if negationWordWeightMax == 0 else negationWordWeightMin / negationWordWeightMax;\n",
    "\n",
    "            m1 = min(min(c1, c2), 1);\n",
    "            m2 = min(min(c3, c4), 1);\n",
    "\n",
    "            result += 0.5*(m1+m2);\n",
    "\n",
    "        result = math.fabs(result / (minLen + maxLen - len(common_words) + EPSILON));\n",
    "        return result;\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Segundo uma porta-voz da ONU, o avião, de fabricação russa, estava tentando aterrissar no aeroporto de Bukavu em meio a uma tempestade.\n",
      "Um acidente aéreo na localidade de Bukavu, no leste da República Democrática do Congo (RDC), matou 17 pessoas na quinta-feira à tarde, informou nesta sexta-feira um porta-voz das Nações Unidas.\n",
      "Um acidente aéreo na localidade de Bukavu, no leste da República Democrática do Congo, matou 17 pessoas na quinta-feira à tarde, informou hoje um porta-voz das Nações Unidas.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "# line = open('output/experiments/C1_Mundo_AviaoCongo/sumario_automatico_CSTSumm.txt').read().decode('utf-8') \n",
    "line = open('experimentos/01/C1_sumario_automatico_CSTSumm.txt').read().decode('utf-8') \n",
    "SENTENCES_COUNT = line.count('.')\n",
    "print(SENTENCES_COUNT)\n",
    "\n",
    "# parser = PlaintextParser.from_file(\"output/experiments/C1_Mundo_AviaoCongo/original.txt\", Tokenizer(LANGUAGE))\n",
    "parser = PlaintextParser.from_file('experimentos/01/C1_original.txt', Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "summarizer = ConceptSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import os, fnmatch\n",
    "# from sumy.parsers.plaintext import PlaintextParser\n",
    "# from sumy.nlp.tokenizers import Tokenizer\n",
    "# from sumy.nlp.stemmers import Stemmer\n",
    "# from sumy.utils import get_stop_words\n",
    "\n",
    "# LANGUAGE = \"english\"\n",
    "# rootdir = 'output'\n",
    "\n",
    "# for subdirs, dirs, files in os.walk(rootdir):\n",
    "#   for dir in dirs:\n",
    "#     for file in os.listdir(os.path.join(subdirs, dir)):\n",
    "#       if fnmatch.fnmatch(file, '*original.txt'): \n",
    "#         line = open(os.path.join(subdirs, dir, 'sumario_automatico_CSTSumm.txt')).read().decode('latin-1')\n",
    "#         SENTENCES_COUNT = line.count('.')\n",
    "#         parser = PlaintextParser.from_file(os.path.join(subdirs, dir, 'original.txt'), Tokenizer(LANGUAGE))\n",
    "#         stemmer = Stemmer(LANGUAGE)\n",
    "#         summarizer = ConceptSummarizer(stemmer)\n",
    "#         summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "#         f = open(os.path.join(subdirs, dir, 'concept_summ.txt'), 'w+')\n",
    "#         for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "#           f.write(str(sentence))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
