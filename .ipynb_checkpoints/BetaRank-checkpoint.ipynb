{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    numpy = None\n",
    "\n",
    "from sumy.summarizers._summarizer import AbstractSummarizer\n",
    "from sumy._compat import Counter\n",
    "\n",
    "\n",
    "class BetaSummarizer(AbstractSummarizer):\n",
    "    \"\"\"\n",
    "    LexRank: Graph-based Centrality as Salience in Text Summarization\n",
    "    Source: http://tangra.si.umich.edu/~radev/lexrank/lexrank.pdf\n",
    "    \"\"\"\n",
    "    threshold = 0.1\n",
    "    epsilon = 0.1\n",
    "    _stop_words = frozenset()\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = frozenset(map(self.normalize_word, words))\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        self._ensure_dependencies_installed()\n",
    "\n",
    "        sentences_words = [self._to_words_set(s) for s in document.sentences]\n",
    "        if not sentences_words:\n",
    "            return tuple()\n",
    "\n",
    "        tf_metrics = self._compute_tf(sentences_words)\n",
    "        idf_metrics = self._compute_idf(sentences_words)\n",
    "\n",
    "        matrix = self._create_matrix(sentences_words, self.threshold, tf_metrics, idf_metrics)\n",
    "        scores = self.power_method(matrix, self.epsilon)\n",
    "        ratings = dict(zip(document.sentences, scores))\n",
    "\n",
    "        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dependencies_installed():\n",
    "        if numpy is None:\n",
    "            raise ValueError(\"LexRank summarizer requires NumPy. Please, install it by command 'pip install numpy'.\")\n",
    "\n",
    "    def _to_words_set(self, sentence):\n",
    "        words = map(self.normalize_word, sentence.words)\n",
    "        return [self.stem_word(w) for w in words if w not in self._stop_words]\n",
    "\n",
    "    def _compute_tf(self, sentences):\n",
    "        tf_values = map(Counter, sentences)\n",
    "\n",
    "        tf_metrics = []\n",
    "        for sentence in tf_values:\n",
    "            metrics = {}\n",
    "            max_tf = self._find_tf_max(sentence)\n",
    "\n",
    "            for term, tf in sentence.items():\n",
    "                metrics[term] = tf / max_tf\n",
    "\n",
    "            tf_metrics.append(metrics)\n",
    "\n",
    "        return tf_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_tf_max(terms):\n",
    "        return max(terms.values()) if terms else 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_idf(sentences):\n",
    "        idf_metrics = {}\n",
    "        sentences_count = len(sentences)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for term in sentence:\n",
    "                if term not in idf_metrics:\n",
    "                    n_j = sum(1 for s in sentences if term in s)\n",
    "                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n",
    "\n",
    "        return idf_metrics\n",
    "\n",
    "    def _create_matrix(self, sentences, threshold, tf_metrics, idf_metrics):\n",
    "        \"\"\"\n",
    "        Creates matrix of shape |sentences|×|sentences|.\n",
    "        \"\"\"\n",
    "        # create matrix |sentences|×|sentences| filled with zeroes\n",
    "        sentences_count = len(sentences)\n",
    "        matrix = numpy.zeros((sentences_count, sentences_count))\n",
    "        degrees = numpy.zeros((sentences_count, ))\n",
    "\n",
    "        for row, (sentence1, tf1) in enumerate(zip(sentences, tf_metrics)):\n",
    "            for col, (sentence2, tf2) in enumerate(zip(sentences, tf_metrics)):\n",
    "                matrix[row, col] = self.compute_distance(sentence1, sentence2, tf1, tf2, idf_metrics)\n",
    "\n",
    "                if matrix[row, col] > threshold:\n",
    "                    matrix[row, col] = 1.0\n",
    "                    degrees[row] += 1\n",
    "                else:\n",
    "                    matrix[row, col] = 0\n",
    "\n",
    "        for row in range(sentences_count):\n",
    "            for col in range(sentences_count):\n",
    "                if degrees[row] == 0:\n",
    "                    degrees[row] = 1\n",
    "\n",
    "                matrix[row][col] = matrix[row][col] / degrees[row]\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    #@staticmethod\n",
    "\n",
    "    @staticmethod\n",
    "    def power_method(matrix, epsilon):\n",
    "        transposed_matrix = matrix.T\n",
    "        sentences_count = len(matrix)\n",
    "        p_vector = numpy.array([1.0 / sentences_count] * sentences_count)\n",
    "        lambda_val = 1.0\n",
    "\n",
    "        while lambda_val > epsilon:\n",
    "            next_p = numpy.dot(transposed_matrix, p_vector)\n",
    "            lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))\n",
    "            p_vector = next_p\n",
    "\n",
    "        return p_vector\n",
    "    @staticmethod\n",
    "    def compute_distance(sentence1, sentence2, tf1, tf2, idf_metrics):\n",
    "        common_words = frozenset(sentence1) & frozenset(sentence2)\n",
    "\n",
    "        numerator = 0.0\n",
    "        for term in common_words:\n",
    "            numerator += tf1[term]*tf2[term] * idf_metrics[term]**2\n",
    "\n",
    "        denominator1 = sum((tf1[t]*idf_metrics[t])**2 for t in sentence1)\n",
    "        denominator2 = sum((tf2[t]*idf_metrics[t])**2 for t in sentence2)\n",
    "\n",
    "        if denominator1 > 0 and denominator2 > 0:\n",
    "            return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n",
    "        else:\n",
    "            return 0.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lula diz que vaia na abertura do Pan é 'reação do ser humano'O presidente Luiz Inácio Lula da Silva classificou de reação do ser humano as vaias que recebeu, na última sexta-feira, durante a abertura dos Jogos Pan-Americanos do Rio, no Maracanã.\n",
      "O Rio de Janeiro a gente poderia dizer continua lindo e merece que o governo federal faça o que for possível para o Rio de Janeiro. Lula diz que ficou triste com vaias durante abertura do PanPresidente diz que vaia na abertura do Pan não muda relação do governo com RioO presidente Luiz Inácio Lula da Silva afirmou nesta segunda-feira, durante o programa de rádio \"Café com o Presidente\", que ficou triste com as vaias que recebeu durante a abertura oficial da 15ª edição dos Jogos Pan-Americanos, realizada no estádio do Maracanã, no Rio de Janeiro.\n",
      "Eu tenho certeza de que não é esse o pensamento do Rio de Janeiro.\n"
     ]
    }
   ],
   "source": [
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 3\n",
    "\n",
    "# parser = PlaintextParser.from_file(\"experimentos/01/C1_original.txt\", Tokenizer(LANGUAGE))\n",
    "parser = PlaintextParser.from_file(\"output/experiments/C49_Cotidiano_VaiaLula/original.txt\", Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "    \n",
    "summarizer = BetaSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "output/experiments/C16_Politica_Sanguessugas/original.txt\n",
      "\n",
      "output/experiments/C6_Cotidiano_CanteiroObras/original.txt\n",
      "\n",
      "output/experiments/C29_Mundo_IndenizacaoIgreja/original.txt\n",
      "\n",
      "output/experiments/C20_Politica_CPMF/original.txt\n",
      "\n",
      "output/experiments/C47_Mundo_TurquiaIraque/original.txt\n",
      "\n",
      "output/experiments/C27_Esportes_GoleadaEquador/original.txt\n",
      "\n",
      "output/experiments/C4_Cotidiano_AlagamentoSP/original.txt\n",
      "\n",
      "output/experiments/C1_Mundo_AviaoCongo/original.txt\n",
      "\n",
      "output/experiments/C44_Polбtica_Renan2 - concordanciaCST/original.txt\n",
      "\n",
      "output/experiments/C18_Mundo_AtaqueVirginia - concordanciaRST/original.txt\n",
      "\n",
      "output/experiments/C9_Politica_Desvio/original.txt\n",
      "\n",
      "output/experiments/C28_Esportes_HeptaVolei/original.txt\n",
      "\n",
      "output/experiments/C36_Cotidiano_MorteACM/original.txt\n",
      "\n",
      "output/experiments/C15_Mundo_ExplosaoMoscou/original.txt\n",
      "\n",
      "output/experiments/C10_Mundo_BombardeioLibano/original.txt\n",
      "\n",
      "output/experiments/C14_Mundo_AcidenteTrens/original.txt\n",
      "\n",
      "output/experiments/C40_Polбtica_ProcuradorRenan/original.txt\n",
      "\n",
      "output/experiments/C11_Cotidiano_PCC/original.txt\n",
      "\n",
      "output/experiments/C34_Cotidiano_MalhaFina/original.txt\n",
      "\n",
      "output/experiments/C3_Cotidiano_AcidenteTAM/original.txt\n",
      "\n",
      "output/experiments/C21_Cotidiano_ReformaCumbica/original.txt\n",
      "\n",
      "output/experiments/C45_Cotidiano_RolexLuciano/original.txt\n",
      "\n",
      "output/experiments/C43_Polбtica_Renan/original.txt\n",
      "\n",
      "output/experiments/C25_Esportes_CopaAmerica/original.txt\n",
      "\n",
      "output/experiments/C50_Polбtica_CPMFPSDB - concordanciaRST/original.txt\n",
      "\n",
      "output/experiments/C24_Esportes_FabianaMue/original.txt\n",
      "\n",
      "output/experiments/C23_Mundo_EnchenteReinoUnido - concordanciaCST/original.txt\n",
      "\n",
      "output/experiments/C42_Polбtica_RelatorRenan - concordanciaCST/original.txt\n",
      "\n",
      "output/experiments/C19_Esportes_Maradona - concordanciaCST/original.txt\n",
      "\n",
      "output/experiments/C38_Esportes_PanNatacao/original.txt\n",
      "\n",
      "output/experiments/C48_Esportes_VaiaBernardinho - concordanciaCST/original.txt\n",
      "\n",
      "output/experiments/C31_Esportes_Jade - concordanciaCST/original.txt\n",
      "\n",
      "output/experiments/C22_Cotidiano_DeslizamentoCongonhas/original.txt\n",
      "\n",
      "output/experiments/C37_Cotidiano_MotimMaranh╞o - concordanciaCST/original.txt\n",
      "\n",
      "output/experiments/C30_Dinheiro_LucroItau - concordanciaRST/original.txt\n",
      "\n",
      "output/experiments/C2_Politica_ReeleicaoLula/original.txt\n",
      "\n",
      "output/experiments/C33_Cotidiano_LulaBio/original.txt\n",
      "\n",
      "output/experiments/C12_Mundo_EnchenteCoreia/original.txt\n",
      "\n",
      "output/experiments/C8_Esportes_LigaVolei/original.txt\n",
      "\n",
      "output/experiments/C39_Cotidiano_PF_Congonhas - concordanciaRST/original.txt\n",
      "\n",
      "output/experiments/C26_Mundo_FuracaoMexico/original.txt\n",
      "\n",
      "output/experiments/C7_Ciencia_NovoPlaneta - concordanciaRST/original.txt\n",
      "\n",
      "output/experiments/C35_Mundo_Megatraficante/original.txt\n",
      "\n",
      "output/experiments/C13_Mundo_SriLanka/original.txt\n",
      "\n",
      "output/experiments/C17_Politica_EleicaoAlckmim/original.txt\n",
      "\n",
      "output/experiments/C46_Mundo_TerremotoJapao/original.txt\n",
      "\n",
      "output/experiments/C32_Mundo_FalhaNuclear/original.txt\n",
      "\n",
      "output/experiments/C41_Esportes_RecordeThiago/original.txt\n",
      "\n",
      "output/experiments/C5_Cotidiano_ANAC - concordanciaCST/original.txt\n",
      "\n",
      "output/experiments/C49_Cotidiano_VaiaLula/original.txt\n"
     ]
    }
   ],
   "source": [
    "import os, fnmatch\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "rootdir = 'output'\n",
    "\n",
    "for subdirs, dirs, files in os.walk(rootdir):\n",
    "  for dir in dirs:\n",
    "    for file in os.listdir(os.path.join(subdirs, dir)):\n",
    "      if fnmatch.fnmatch(file, '*original.txt'): \n",
    "        line = open(os.path.join(subdirs, dir, 'sumario_automatico_CSTSumm.txt')).read().decode('latin-1')\n",
    "        SENTENCES_COUNT = line.count('.')\n",
    "        parser = PlaintextParser.from_file(os.path.join(subdirs, dir, 'original.txt'), Tokenizer(LANGUAGE))\n",
    "        stemmer = Stemmer(LANGUAGE)\n",
    "        summarizer = BetaSummarizer(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "        print(\"\\n\"+os.path.join(subdirs, dir, 'original.txt'))\n",
    "        f = open(os.path.join(subdirs, dir, 'lexrank_summ.txt'), 'w+')\n",
    "        for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "          print(str(sentence))\n",
    "          f.write(str(sentence))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
