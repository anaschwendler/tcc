{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    numpy = None\n",
    "\n",
    "from sumy.summarizers._summarizer import AbstractSummarizer\n",
    "from sumy._compat import Counter\n",
    "\n",
    "\n",
    "class BetaSummarizer(AbstractSummarizer):\n",
    "    \"\"\"\n",
    "    LexRank: Graph-based Centrality as Salience in Text Summarization\n",
    "    Source: http://tangra.si.umich.edu/~radev/lexrank/lexrank.pdf\n",
    "    \"\"\"\n",
    "    threshold = 0.1\n",
    "    epsilon = 0.1\n",
    "    _stop_words = frozenset()\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = frozenset(map(self.normalize_word, words))\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        self._ensure_dependencies_installed()\n",
    "\n",
    "        sentences_words = [self._to_words_set(s) for s in document.sentences]\n",
    "        if not sentences_words:\n",
    "            return tuple()\n",
    "\n",
    "        tf_metrics = self._compute_tf(sentences_words)\n",
    "        idf_metrics = self._compute_idf(sentences_words)\n",
    "\n",
    "        matrix = self._create_matrix(sentences_words, self.threshold, tf_metrics, idf_metrics)\n",
    "        scores = self.power_method(matrix, self.epsilon)\n",
    "        ratings = dict(zip(document.sentences, scores))\n",
    "\n",
    "        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dependencies_installed():\n",
    "        if numpy is None:\n",
    "            raise ValueError(\"LexRank summarizer requires NumPy. Please, install it by command 'pip install numpy'.\")\n",
    "\n",
    "    def _to_words_set(self, sentence):\n",
    "        words = map(self.normalize_word, sentence.words)\n",
    "        return [self.stem_word(w) for w in words if w not in self._stop_words]\n",
    "\n",
    "    def _compute_tf(self, sentences):\n",
    "        tf_values = map(Counter, sentences)\n",
    "\n",
    "        tf_metrics = []\n",
    "        for sentence in tf_values:\n",
    "            metrics = {}\n",
    "            max_tf = self._find_tf_max(sentence)\n",
    "\n",
    "            for term, tf in sentence.items():\n",
    "                metrics[term] = tf / max_tf\n",
    "\n",
    "            tf_metrics.append(metrics)\n",
    "\n",
    "        return tf_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_tf_max(terms):\n",
    "        return max(terms.values()) if terms else 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_idf(sentences):\n",
    "        idf_metrics = {}\n",
    "        sentences_count = len(sentences)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for term in sentence:\n",
    "                if term not in idf_metrics:\n",
    "                    n_j = sum(1 for s in sentences if term in s)\n",
    "                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n",
    "\n",
    "        return idf_metrics\n",
    "\n",
    "    def _create_matrix(self, sentences, threshold, tf_metrics, idf_metrics):\n",
    "        \"\"\"\n",
    "        Creates matrix of shape |sentences|×|sentences|.\n",
    "        \"\"\"\n",
    "        # create matrix |sentences|×|sentences| filled with zeroes\n",
    "        sentences_count = len(sentences)\n",
    "        matrix = numpy.zeros((sentences_count, sentences_count))\n",
    "        degrees = numpy.zeros((sentences_count, ))\n",
    "\n",
    "        for row, (sentence1, tf1) in enumerate(zip(sentences, tf_metrics)):\n",
    "            for col, (sentence2, tf2) in enumerate(zip(sentences, tf_metrics)):\n",
    "                matrix[row, col] = compute_distance(sentence1, sentence2, tf1, tf2, idf_metrics)\n",
    "\n",
    "                if matrix[row, col] > threshold:\n",
    "                    matrix[row, col] = 1.0\n",
    "                    degrees[row] += 1\n",
    "                else:\n",
    "                    matrix[row, col] = 0\n",
    "\n",
    "        for row in range(sentences_count):\n",
    "            for col in range(sentences_count):\n",
    "                if degrees[row] == 0:\n",
    "                    degrees[row] = 1\n",
    "\n",
    "                matrix[row][col] = matrix[row][col] / degrees[row]\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    #@staticmethod\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def power_method(matrix, epsilon):\n",
    "        transposed_matrix = matrix.T\n",
    "        sentences_count = len(matrix)\n",
    "        p_vector = numpy.array([1.0 / sentences_count] * sentences_count)\n",
    "        lambda_val = 1.0\n",
    "\n",
    "        while lambda_val > epsilon:\n",
    "            next_p = numpy.dot(transposed_matrix, p_vector)\n",
    "            lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))\n",
    "            p_vector = next_p\n",
    "\n",
    "        return p_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##ALTERAR AQUI\n",
    "\n",
    "def compute_distance(sentence1, sentence2, tf1, tf2, idf_metrics):\n",
    "    # RETURN METODO_WIVES_AQUI()   \n",
    "    EPSILON = 0.0000000000000001\n",
    "    result = 0\n",
    "    \n",
    "    # identify common words\n",
    "    common_words = frozenset(sentence1) & frozenset(sentence2)\n",
    "    \n",
    "    if len(sentence1) > len(sentence2): \n",
    "        maxLen = len(sentence1); \n",
    "        minLen = len(sentence2) \n",
    "    else: \n",
    "        maxLen = len(sentence2); \n",
    "        minLen = len(sentence1) \n",
    "    \n",
    "    # calculates similarity\n",
    "    wordWeightMax = 0; wordWeightMin = 0;\n",
    "    for term in common_words:\n",
    "        if wordWeightMax < len(term): wordWeightMax = len(term)\n",
    "        if wordWeightMin > len(term): wordWeightMin = len(term)\n",
    "        negationWordWeightMax = 1 - wordWeightMax;\n",
    "        negationWordWeightMin = 1 - wordWeightMin;\n",
    "            \n",
    "        c1 = 1 if wordWeightMin == 0 else wordWeightMax / wordWeightMin;\n",
    "        c2 = 1 if wordWeightMax == 0 else wordWeightMin / wordWeightMax;\n",
    "        c3 = 1 if negationWordWeightMin == 0 else negationWordWeightMax / negationWordWeightMin;\n",
    "        c4 = 1 if negationWordWeightMax == 0 else negationWordWeightMin / negationWordWeightMax;\n",
    "\n",
    "        m1 = min(min(c1, c2), 1);\n",
    "        m2 = min(min(c3, c4), 1);\n",
    "\n",
    "        result += 0.5*(m1+m2);\n",
    "    \n",
    "    result =math.fabs(result / (minLen + maxLen - len(common_words) + EPSILON));\n",
    "#     print(result)\n",
    "    return result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##ALTERAR AQUI\n",
    "\n",
    "def compute_old(sentence1, sentence2, tf1, tf2, idf_metrics):\n",
    "    # RETURN METODO_WIVES_AQUI()   \n",
    "    \n",
    "    common_words = frozenset(sentence1) & frozenset(sentence2)\n",
    "    \n",
    "    print(common_words)\n",
    "\n",
    "    numerator = 0.0\n",
    "    for term in common_words:\n",
    "        numerator += tf1[term]*tf2[term] * idf_metrics[term]**2\n",
    "\n",
    "    denominator1 = sum((tf1[t]*idf_metrics[t])**2 for t in sentence1)\n",
    "    denominator2 = sum((tf2[t]*idf_metrics[t])**2 for t in sentence2)\n",
    "\n",
    "    if denominator1 > 0 and denominator2 > 0:\n",
    "        return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queda de avião mata 17 pessoas no Congo.\n",
      "Ao menos 17 pessoas morreram após a queda de um avião de passageiros na República Democrática do Congo.\n",
      "Segundo uma porta-voz da ONU, o avião, de fabricação russa, estava tentando aterrissar no aeroporto de Bukavu em meio a uma tempestade.\n",
      "Um acidente aéreo na localidade de Bukavu, no leste da República Democrática do Congo (RDC), matou 17 pessoas na quinta-feira à tarde, informou nesta sexta-feira um porta-voz das Nações Unidas.\n",
      "Um acidente aéreo na localidade de Bukavu, no leste da República Democrática do Congo, matou 17 pessoas na quinta-feira à tarde, informou hoje um porta-voz das Nações Unidas.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 5\n",
    "\n",
    "\n",
    "parser = PlaintextParser.from_file(\"experimentos/01/C1_original.txt\", Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "    \n",
    "summarizer = BetaSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
