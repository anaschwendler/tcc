{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    numpy = None\n",
    "\n",
    "from sumy.summarizers._summarizer import AbstractSummarizer\n",
    "from sumy._compat import Counter\n",
    "\n",
    "\n",
    "class BetaSummarizer(AbstractSummarizer):\n",
    "    \"\"\"\n",
    "    LexRank: Graph-based Centrality as Salience in Text Summarization\n",
    "    Source: http://tangra.si.umich.edu/~radev/lexrank/lexrank.pdf\n",
    "    \"\"\"\n",
    "    threshold = 0.1\n",
    "    epsilon = 0.1\n",
    "    _stop_words = frozenset()\n",
    "\n",
    "    @property\n",
    "    def stop_words(self):\n",
    "        return self._stop_words\n",
    "\n",
    "    @stop_words.setter\n",
    "    def stop_words(self, words):\n",
    "        self._stop_words = frozenset(map(self.normalize_word, words))\n",
    "\n",
    "    def __call__(self, document, sentences_count):\n",
    "        self._ensure_dependencies_installed()\n",
    "\n",
    "        sentences_words = [self._to_words_set(s) for s in document.sentences]\n",
    "        if not sentences_words:\n",
    "            return tuple()\n",
    "\n",
    "        tf_metrics = self._compute_tf(sentences_words)\n",
    "        idf_metrics = self._compute_idf(sentences_words)\n",
    "\n",
    "        matrix = self._create_matrix(sentences_words, self.threshold, tf_metrics, idf_metrics)\n",
    "        scores = self.power_method(matrix, self.epsilon)\n",
    "        ratings = dict(zip(document.sentences, scores))\n",
    "\n",
    "        return self._get_best_sentences(document.sentences, sentences_count, ratings)\n",
    "\n",
    "    @staticmethod\n",
    "    def _ensure_dependencies_installed():\n",
    "        if numpy is None:\n",
    "            raise ValueError(\"LexRank summarizer requires NumPy. Please, install it by command 'pip install numpy'.\")\n",
    "\n",
    "    def _to_words_set(self, sentence):\n",
    "        words = map(self.normalize_word, sentence.words)\n",
    "        return [self.stem_word(w) for w in words if w not in self._stop_words]\n",
    "\n",
    "    def _compute_tf(self, sentences):\n",
    "        tf_values = map(Counter, sentences)\n",
    "\n",
    "        tf_metrics = []\n",
    "        for sentence in tf_values:\n",
    "            metrics = {}\n",
    "            max_tf = self._find_tf_max(sentence)\n",
    "\n",
    "            for term, tf in sentence.items():\n",
    "                metrics[term] = tf / max_tf\n",
    "\n",
    "            tf_metrics.append(metrics)\n",
    "\n",
    "        return tf_metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def _find_tf_max(terms):\n",
    "        return max(terms.values()) if terms else 1\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_idf(sentences):\n",
    "        idf_metrics = {}\n",
    "        sentences_count = len(sentences)\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for term in sentence:\n",
    "                if term not in idf_metrics:\n",
    "                    n_j = sum(1 for s in sentences if term in s)\n",
    "                    idf_metrics[term] = math.log(sentences_count / (1 + n_j))\n",
    "\n",
    "        return idf_metrics\n",
    "\n",
    "    def _create_matrix(self, sentences, threshold, tf_metrics, idf_metrics):\n",
    "        \"\"\"\n",
    "        Creates matrix of shape |sentences|×|sentences|.\n",
    "        \"\"\"\n",
    "        # create matrix |sentences|×|sentences| filled with zeroes\n",
    "        sentences_count = len(sentences)\n",
    "        matrix = numpy.zeros((sentences_count, sentences_count))\n",
    "        degrees = numpy.zeros((sentences_count, ))\n",
    "\n",
    "        for row, (sentence1, tf1) in enumerate(zip(sentences, tf_metrics)):\n",
    "            for col, (sentence2, tf2) in enumerate(zip(sentences, tf_metrics)):\n",
    "                matrix[row, col] = compute_distance(sentence1, sentence2, tf1, tf2, idf_metrics)\n",
    "\n",
    "                if matrix[row, col] > threshold:\n",
    "                    matrix[row, col] = 1.0\n",
    "                    degrees[row] += 1\n",
    "                else:\n",
    "                    matrix[row, col] = 0\n",
    "\n",
    "        for row in range(sentences_count):\n",
    "            for col in range(sentences_count):\n",
    "                if degrees[row] == 0:\n",
    "                    degrees[row] = 1\n",
    "\n",
    "                matrix[row][col] = matrix[row][col] / degrees[row]\n",
    "\n",
    "        return matrix\n",
    "\n",
    "    #@staticmethod\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def power_method(matrix, epsilon):\n",
    "        transposed_matrix = matrix.T\n",
    "        sentences_count = len(matrix)\n",
    "        p_vector = numpy.array([1.0 / sentences_count] * sentences_count)\n",
    "        lambda_val = 1.0\n",
    "\n",
    "        while lambda_val > epsilon:\n",
    "            next_p = numpy.dot(transposed_matrix, p_vector)\n",
    "            lambda_val = numpy.linalg.norm(numpy.subtract(next_p, p_vector))\n",
    "            p_vector = next_p\n",
    "\n",
    "        return p_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##ALTERAR AQUI\n",
    "\n",
    "def compute_distance(sentence1, sentence2, tf1, tf2, idf_metrics):\n",
    "    # RETURN METODO_WIVES_AQUI()   \n",
    "    \n",
    "    # identify common words\n",
    "    common_words = frozenset(sentence1) & frozenset(sentence2)\n",
    "    \n",
    "    # calculates similarity\n",
    "    wordWeightMax = 0; wordWeightMin = 0;\n",
    "    for term in common_words:\n",
    "        if wordWeightMax < len(term): wordWeightMax = len(term)\n",
    "        if wordWeightMin > len(term): wordWeightMin = len(term)\n",
    "        negationWordWeightMax = 1 - wordWeightMax;\n",
    "        negationWordWeightMin = 1 - wordWeightMin;\n",
    "            \n",
    "        c1 = 1 if wordWeightMin == 0 else wordWeightMax / wordWeightMin;\n",
    "        c2 = 1 if wordWeightMax == 0 else wordWeightMin / wordWeightMax;\n",
    "        c3 = 1 if negationWordWeightMin == 0 else negationWordWeightMax / negationWordWeightMin;\n",
    "        c4 = 1 if negationWordWeightMax == 0 else negationWordWeightMin / negationWordWeightMax;\n",
    "\n",
    "        m1 = Math.min(Math.min(c1, c2), 1);\n",
    "        m2 = Math.min(Math.min(c3, c4), 1);\n",
    "\n",
    "        result+=0.5*(m1+m2);\n",
    "    \n",
    "    #duvida aqui, preciso pensar melhor\n",
    "    result = result / (min.nWords() + max.nWords() - commonWords.size() + EPSILON);\n",
    "    return result;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##ALTERAR AQUI\n",
    "\n",
    "def compute_old(sentence1, sentence2, tf1, tf2, idf_metrics):\n",
    "    # RETURN METODO_WIVES_AQUI()   \n",
    "    \n",
    "    common_words = frozenset(sentence1) & frozenset(sentence2)\n",
    "    \n",
    "    print(common_words)\n",
    "\n",
    "    numerator = 0.0\n",
    "    for term in common_words:\n",
    "        numerator += tf1[term]*tf2[term] * idf_metrics[term]**2\n",
    "\n",
    "    denominator1 = sum((tf1[t]*idf_metrics[t])**2 for t in sentence1)\n",
    "    denominator2 = sum((tf2[t]*idf_metrics[t])**2 for t in sentence2)\n",
    "\n",
    "    if denominator1 > 0 and denominator2 > 0:\n",
    "        return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n",
    "    else:\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozenset([u'eletr\\xf4nico', u'para', u'pok\\xe9mon', u'de', u'realidad', u'smartphon', u'aumentada', u'um', u'\\xe9', u'voltado', u'jogo'])\n",
      "frozenset([u'pok\\xe9mon', u'para'])\n",
      "frozenset([u'de', u'jogo'])\n",
      "frozenset([u'pok\\xe9mon', u'de', u'jogo'])\n",
      "frozenset([u'pok\\xe9mon', u'um', u'para'])\n",
      "frozenset([u'pok\\xe9mon', u'para'])\n",
      "frozenset([u'niantic', u'para', u'pok\\xe9mon', u'por', u'plataforma', u'desenvolvido', u'entr', u'uma', u'io', u'nintendo', u'colabora\\xe7\\xe3o', u'compani', u'foi', u'android'])\n",
      "frozenset([u'foi'])\n",
      "frozenset([u'pok\\xe9mon'])\n",
      "frozenset([u'pok\\xe9mon', u'para'])\n",
      "frozenset([u'de', u'jogo'])\n",
      "frozenset([u'foi'])\n",
      "frozenset([u'em', u'lan\\xe7ado', u'julho', u'algun', u'de', u'pa\\xeds', u'mundo', u'jogo', u'foi'])\n",
      "frozenset([u'de', u'mundo', u'jogo'])\n",
      "frozenset([])\n",
      "frozenset([u'pok\\xe9mon', u'de', u'jogo'])\n",
      "frozenset([u'pok\\xe9mon'])\n",
      "frozenset([u'de', u'mundo', u'jogo'])\n",
      "frozenset([u'treinar', u'fossem', u'jogador', u'ao', u'batalhar', u'jogo', u'capturar', u'fazendo', u'uso', u'virtuai', u'tela', u'nas', u'dispositivo', u'gps', u'real', u'chamada', u'de', u'aparecem', u'compat\\xedvei', u'mundo', u'como', u'criatura', u'pok\\xe9mon', u'c\\xe2mera', u'permit', u'se'])\n",
      "frozenset([u'pok\\xe9mon', u'nas', u'dispositivo'])\n",
      "frozenset([u'pok\\xe9mon', u'um', u'para'])\n",
      "frozenset([u'pok\\xe9mon', u'para'])\n",
      "frozenset([])\n",
      "frozenset([u'pok\\xe9mon', u'nas', u'dispositivo'])\n",
      "frozenset([u'est\\xe1', u'para', u'usu\\xe1rio', u'vest\\xedvel', u'dispositivo', u'quando', u'previsto', u'pok\\xe9mon', u'estiverem', u'um', u'nas', u'futuro', u'opcion', u'ir\\xe1', u'alertar', u'proximidad', u'os', u'lan\\xe7amento'])\n",
      "Pokémon GO é um jogo eletrônico free-to-play de realidade aumentada voltado para smartphones.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 1\n",
    "\n",
    "\n",
    "parser = PlaintextParser.from_file(\"go.txt\", Tokenizer(LANGUAGE))\n",
    "stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "summarizer = BetaSummarizer(stemmer)\n",
    "summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "\n",
    "for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
